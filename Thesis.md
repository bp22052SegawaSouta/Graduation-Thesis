# 大規模言語モデルを用いたドメインモデル生成における網羅性向上とベストプラクティスの組み込み

## 初めに
### 研究背景
現在のソフトウェア開発は複雑化の一途を辿っており、その解決策としてEric Evansによって提唱されたドメイン駆動設計(DDD: Domain-Driven Design)が注目されている。（ここに引用E. Evans, Domain-Driven Design: Tackling Complexity in the
Heart of Software, Addison-Wesley Professional, 2003.）

DDDは、ソフトウェア開発をより効率的かつ顧客のニーズに合致させる強力な手法である一方で、その中核をなすドメインモデルの作成は、非常に難易度が高く、多大な時間と労力を要するという課題がある。この課題に対し、古くからドメイン知識を含む文書を自然言語処理技術で分析し、モデリングを補助する研究が盛んにおこなわれてきた。近年では、大規模言語モデル(LLM: Large Language Model)を用いてドメインモデルを生成する研究も進められている。しかし、現状のLLMが生成するモデルは完全ではなく、特に要素の網羅性の低さや、モデリングにおけるベストプラクティスの欠如が大きな課題として挙げられている。本研究では、これらの課題を解決するため。LLMベースのAIエージェントを開発し、出力されるドメインモデルの要素網羅性を向上させ、モデリングのベストプラクティスを組み込むことを目指す。

### 研究課題
LLMによるドメインモデルの生成における課題は大きく二つある。一つは生成したモデルの要素の網羅性が低いということである。とくに与えた情報にあいまいな部分が発生した場合、該当箇所に関係する部分を無視してモデルを生成してしまうといった問題が発生している。

もう一つは出力されたモデルがモデリングにおけるベストプラクティスを無視してしまうといったものである。

### 研究目的
本研究の目的はLLMを利用したモデル生成において課題とされていた要素の網羅性の低さとベストプラクティスの組み込みを行うことである。この目的の達成のためLLMベースのAIエージェントを作成し、モデル作成を支援できるようなツールを作成する。

### 本稿の構成

## 関連研究
### ドメイン駆動設計
Eric Evansによって提唱された設計手法

### LLMベースのモデリングにおける質問分解アプローチの適用
#### 概要
文献（？）では大規模言語モデル(LLM: Large Language Model)を用いてドメインモデルを自動生成するための新しいアプローチとして質問分解というアプローチを提案している。このアプローチは従来の単一のプロンプトでモデル全体を生成しようとする手法において課題とされていた、複雑なタスクに対応しきれないという課題や指示の一部を無視してしまうことによる生成モデルの精度の低さに対して、細かくタスク分けを行うことによって改善を図っている。


#### 提案手法
この論文で提案されている質問分解を用いたアプローチの核心は、複雑なドメインモデルの生成というタスクを、エンジニアが実際にモデリングを行う際の思考プロセスに倣って、LLMが比較的解決しやすい粒度にタスクを分割するという点にある。
提案手法は大きく分け以下の４つのステップで構成されている。

#### 1 クラスと属性の生成:システムの使用所から、モデルの基本要素となるクラスと属性を特定
#### 2 クラスの抽出:ステップ1で生成されたテキストからクラス名をプログラムで正確に抽出
#### 3 関連の生成:抽出されたクラスを基に、クラス間の関係性(関連、集約、継承)を生成
#### 4 結合とモデル生成:すべてのステップの結果を統合し、最終的なクラスモデルをプログラムで構築

引用figure2を張り付ける

#### 評価
質問分解アプローチでは従来の単一プロンプト手法と比較して、モデル要素の再現率(Recall)と総合的な精度(F1スコア)が大幅に向上した。また、プロンプトに専門知識を導入することによって生成品質が向上するという結果も得られた。

#### 課題
この提案手法においてF1スコアは大幅に向上したものの、実務でそのまま利用できるレベルにはまだ達しておらず、さらなる改善が必要であるとされている。また、初期のステップにおいて間違いが発生した場合、その後のステップにも悪影響を与えてしまうといった課題が残されていた。



### LLMを用いたドメインモデリングの完全自動化
#### 概要
文献（？）では、LLM（GPT-3.5及びGPT-4）が人間による操作なしで、テキスト形式の要求仕様からどの程度高品質なドメインモデル(クラス、属性、関連)を自動生成できるかを評価することを目的としている。

#### 提案手法
文献(?)ではLLMのモデリング能力の評価のため、出力形式の定義を行い。その後LLMに与えるプロンプトを工夫し、以下の３つの主要な手法を比較検討した。
#### 1 ゼロショット(Zero-shot)
LLMに一切の例を与えずに、タスクの目的と出力形式の指示と対象となる問題記述のみを与える手法。最もシンプルで、LLMが持つ汎用的な能力を試せる。
#### 2 Nショット(N-shot)
タスクの指示と記述問題に加え、N個の「問題記述とそれに対応する正解ドメインモデル」のペアを具体例としてプロンプトに含める手法。これによってLLMは具体的な入出力の形式と内容を学習し、より精度の高い出力を目指す。本論文では1-shotと2-shotが実験された
#### 3 Chain-of-Thought(CoT)
N-shotを発展させた手法。単に問題と最終的な答えのペアを示すのではなく、問題記述の各分が、ドメインモデルのどの要素(クラス、属性、関連など)に結び付くかという中間的な思考プロセスを例に含めて行う。これによって、LLMが複雑なタスクを段階的に解決する方法を学習し、性能が向上することが期待される。
この３つの手法をそれぞれGPT-3.5とGPT-4の二つのモデルで実験した。しかし、各出力があらかじめ定義した出力形式に従うとは限らないため、後処理を行っている。

#### 評価
本論文ではクラス、属性、関連の３つの要素それぞれについて、生成されたモデルと専門家による参照モデルを比較して、以下の３つの定量的指標をもって評価している。
#### 1 適合率(Precision)
生成された要素のうち、どれだけが正しかったか
#### 2 再現率(Recall)
正解モデルに含まれる要素のうち、どれだけを正しく生成(網羅)できたか。
#### 3 F1スコア
適合率と再現率の調和平均
この３つの指標を使い、各RQに対して分析を行っている。
RQ1：LLMは自動ドメインモデリングをどの程度うまく実行できるか
については現状では完全自動化は実用的でないという結論になった。全体として適合率は高かったものの再現率が低く、特に関連の特定が著しく苦手であることが示された。
RQ2:プロンプトエンジニアリングの違いによる影響
例の提示は有効である者のChain-of-Thoughtは逆効果になるという結果になった。これに対して著者はドメインモデリングがテキスト全体を包括的に理解する必要があり、この特性がChain-of-Thoughtと相性が悪かった可能性があると分析している。
RQ3：評価したLLMの中ではどれが最も高性能だったか
結論としてGPT-4が高性能であった。どの要素においてもGPT-3.5を上回る結果となった。
